# Databricks

## Replicate

To replicate snapshot and incremental data of a TiDB Table to Databricks:

```shell
export AWS_ACCESS_KEY_ID=<ACCESS_KEY>
export AWS_SECRET_ACCESS_KEY=<SECRET_KEY>

./tidb2dw databricks \
    --storage s3://my-demo-bucket/prefix \
    --table <database_name>.<table_name> \
    --databricks.host dbc-********-****.cloud.databricks.com \
    --databricks.endpoint 2**************4 \
    --databricks.catalog <catalog> \
    --databricks.schema <schema> \
    --databricks.token dapi********************************


# Note that you may also need to specify these parameters:
#   --cdc.host x.x.x.x
#   --tidb.host x.x.x.x
#   --tidb.user <user>
#   --tidb.pass <pass>
# Use --help for details.
```

## Supported DDL Operations

All DDL which will change the schema of table are supported (except index related), including:

- Add column
- Drop column
- Rename column
- Modify column type
- Drop table
- Truncate table

## Noteworthy

1. [How to give Databricks sufficient permission in AWS](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html)
1. `tidb2dw` uses [AWS STS tokens](https://docs.aws.amazon.com/AmazonS3/latest/userguide/AuthUsingTempSessionToken.html) to read data from AWS S3. And this token is generated by your AWS access key and secret key. So please ensure this account have the “s3:GetObject*”, “s3:ListBucket”, and “s3:GetBucketLocation” permissions.
1. The type mapping from TiDB to Databricks is defined [here](/pkg/databrickssql/types.go).
1. Databricks has some limitations on modifying table schemas, like Databricks does [not support primary key and foreign key](https://docs.databricks.com/en/tables/constraints.html#declare-primary-key-and-foreign-key-relationships), not support default value in all kind of storage layers yet. 
